
//**************INSTALLATION OF DEPENDENCIES****************//

This code requires fftw3 with mpi. To download and install, 
go to any directory you like and do the follwing,

mkdir dependencies
cd dependencies
wget https://www.fftw.org/fftw-3.3.10.tar.gz
tar -xvzf fftw-3.3.10.tar.gz
mkdir fftw3_install
cd fftw3_install
export install_dir = $PWD (or manually copy output of pwd	) 
cd ../fftw-3.3.10
./configure --enable-shared --enable-float --enable-threads --enable-mpi --prefix=$install_dir
make -j 4
make install

(Now install once more without --enable-float this will install fftw for double precision)

./configure --enable-shared --enable-threads --enable-mpi --prefix=$install_dir
make -j 4
make install

(In case MPI is installed in standard location just add following lines to .bashrc 
or write a textfile named 'libpaths' and run 'source libpaths')

export LD_LIBRARY_PATH=(path to fftw3_install)/lib:$LD_LIBRARY_PATH
export PATH=(path to fftw3_install)/bin:$PATH

	
(In case MPI is not installed in standard location, you need to provide it's install path in Makefile
To install MPI in a non standard location do the following)

cd dependencies
wget http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz
mkdir mpi_install
cd mpi_install
export install_dir = $PWD (or manually copy output of pwd	) 
cd ../mpich-3.3.2
./configure --prefix=$install_dir
make -j 4 
make install

(Now add following lines to .bashrc or write a textfile named 
'libpaths' and run 'source libpaths')

export LD_LIBRARY_PATH=(path to fftw3_install)/lib:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=(path to mpi_install)/lib:$LD_LIBRARY_PATH

export PATH=(path to fftw3_install)/bin:$PATH
export PATH=(path to mpi_install)/bin:$PATH


//***************************************************//


//**************RUNNING THE CODE ON Cluster********************//


(1) On the ICTS cluster you could simply run the code using the "submit.sh" bash script. 

(2) Number of cpu per process, number of nodes and number of MPI process per node respectively can be set 
    in the submit.sh file by setting their corresponding varialbles. E.g. 
	 
	  "SBATCH --cpus-per-task=16" (sets number of cpu per task to 16 by setting the variable "SLURM_CPUS_PER_TASK")
	  "SBATCH --nodes=4" (sets number of nodes to 4)
	  "SBATCH --ntasks-per-node=2" (sets number of MPI process(task) per node)

		The total number of MPI tasks across nodes has to be product of "nodes" and "ntasks-per-node" 
		i.e. for the given example, it should be set as 
		
		"#SBATCH --ntasks=8" (sets the total number of MPI tasks by setting the variable "SLURM_NPROCS")  	  
	  
(3) Please also make sure that total time for simulation and allocates memory allow for the simulation
    to complete.

(4) The run command then takes all the values set in (2) and runs the code. It also writes the output of the 
    simulation in the "simulation.log" file and timing information in "SimulationDetails.log" file. 

(5) The program has been updated to read/write Vel files by combining data from all the processes. And without any 
    modification the code with behave as such.

(6) However in case of large data it is better to read/write many small files rather than on big file. In order to 
    read/write Vel files seperately by each process, just set the variable "data_scatter" in the beginning of
    spectral_serial.f90 to a non zero value.
    
           
//**************RUNNING THE CODE ON Local computer********************//
				 

(1) Set the number of openmp threads to (say) 8 by

export OMP_NUM_THREADS=8

(2) In order to run the executible with (say) 4 number of processsors do 

mpirun -np 4 ./part.exe (path to the output directory, which should also contain flu.in)

(3) Extremely important to run 'killall part.exe' after you terminate the code
    with keyboard interrupt (say with 'Ctrl+C' or 'Ctrl+Z'). Otherwise some 
    processes may keep running in the background. As an alternative you 
    could run it via the bash script 'traprun.sh' by 'bash traprun.sh'. This can 
    catch 'Ctrl+C' interrupt but not 'Ctrl+Z' so be careful with that.
    
(4) Also run 'killall part.exe' in case the code encounters MPI communicator errors.    


//**************Simulation Details********************//     	






















